{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03cb6fa0-6873-40bf-8e39-7a7cd66b31c3",
      "metadata": {
        "id": "03cb6fa0-6873-40bf-8e39-7a7cd66b31c3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function that extracts features for neighbors based on the neighbor number, passed\n",
        "# latitude and longitude, name (city refernece point). Also passes rows, which is the\n",
        "# collection of data to export to CSV after collecting all data\n",
        "def extract_neighbor_data(latitude, longitude, neighbor_number, name, rows):\n",
        "  params = {\n",
        "    'start': '20000101',\n",
        "    'end': '20221231',\n",
        "    'latitude': latitude,\n",
        "    'longitude': longitude,\n",
        "    'community': 'AG',\n",
        "    'parameters': 'WS2M,T2MDEW,ALLSKY_SFC_LW_DWN,ALLSKY_SFC_SW_DWN,T2M_MAX,T2M_MIN',\n",
        "    'format': 'JSON'\n",
        "  }\n",
        "\n",
        "  # API call\n",
        "  response = requests.get(base_url, params=params)\n",
        "  data = response.json()\n",
        "\n",
        "  # Set features we select (not exported from NASA DAV)\n",
        "  header = ['Date', 'Neighbor Number', 'Reference', 'Latitude', 'Longitude']\n",
        "\n",
        "  # Extract the feature names (keys in 'parameter') for the columns\n",
        "  for feature_name in data['properties']['parameter'].keys():\n",
        "    header.append(feature_name)\n",
        "  header.append(\"FROST\")\n",
        "\n",
        "  # Find all the unique dates by iterating over the parameter data\n",
        "  dates = set()\n",
        "  for feature_name, date_val_dict in data['properties']['parameter'].items():\n",
        "    for date in date_val_dict.keys():\n",
        "      dates.add(date)\n",
        "\n",
        "  # Sort dates to maintain chronological order\n",
        "  dates = sorted(dates)\n",
        "\n",
        "  # For each date, create a row with values for each feature\n",
        "  for date in dates:\n",
        "    # Fill in date, neighbor number, reference name, latitude, and longitude values\n",
        "    row = [date, neighbor_number, name, latitude, longitude]\n",
        "    # Add features to data\n",
        "    for feature_name in data['properties']['parameter'].keys():\n",
        "        # Get the value for the feature for the current date\n",
        "        value = data['properties']['parameter'][feature_name].get(date, None)\n",
        "        row.append(value)\n",
        "\n",
        "        # Now we want to add the frost prediction output\n",
        "        if feature_name == 'T2M_MIN':\n",
        "            t2m = value\n",
        "        elif feature_name == 'T2MDEW':\n",
        "            dew = value\n",
        "\n",
        "    # Check if dew < 0, and if dew < t2m\n",
        "    if t2m < 0 and dew > t2m and dew < 0:\n",
        "        frost = 1\n",
        "    else:\n",
        "        frost = 0\n",
        "\n",
        "    # Add frost prediction to the row\n",
        "    row.append(frost)\n",
        "    # Add instance to the API query dataset\n",
        "    rows.append(row)\n",
        "\n",
        "  # Return all new data that was added on top of old data\n",
        "  return rows\n",
        "\n",
        "# Change directory to where you want files saved\n",
        "os.chdir(\"MY_DIRECTORY\")\n",
        "\n",
        "# API URL\n",
        "base_url = 'https://power.larc.nasa.gov/api/temporal/daily/point'\n",
        "\n",
        "# Locations to query for: latitude, longitude, and city reference name\n",
        "locations = [(35.7796, -78.6382, \"Raleigh\"), \\\n",
        "             (38.9072, -77.0369, \"DC\"), \\\n",
        "             (42.355, -71.0565, \"Boston\"), \\\n",
        "             (34.0008, -81.0351, \"Columbia\"), \\\n",
        "             (34.7501, -84.3885, \"Atlanta\"), \\\n",
        "             (40.7182, -74.006, \"NYC\"), \\\n",
        "             (39.7691, -86.158, \"Indianapolis\"), \\\n",
        "             (29.7601, -95.3701, \"Houston\")]\n",
        "\n",
        "# Go through each location set to query\n",
        "for loc in locations:\n",
        "  # Assign latitude, longitude, and name of city\n",
        "  latitude = loc[0]\n",
        "  longitude = loc[1]\n",
        "  name = loc[2]\n",
        "\n",
        "  # Define parameters for query\n",
        "  params = {\n",
        "    'start': '20000101',\n",
        "    'end': '20221231',\n",
        "    'latitude': latitude,\n",
        "    'longitude': longitude,\n",
        "    'community': 'AG',\n",
        "    'parameters': 'WS2M,T2MDEW,ALLSKY_SFC_LW_DWN,ALLSKY_SFC_SW_DWN,T2M_MAX,T2M_MIN',\n",
        "    'format': 'JSON'\n",
        "  }\n",
        "\n",
        "  # API call\n",
        "  response = requests.get(base_url, params=params)\n",
        "  data = response.json()\n",
        "\n",
        "  # Set features we select (not exported from NASA DAV)\n",
        "  header = ['Date', 'Neighbor Number', 'Reference', 'Latitude', 'Longitude']\n",
        "  # Extract the feature names (keys in 'parameter') for the columns\n",
        "  for feature_name in data['properties']['parameter'].keys():\n",
        "    header.append(feature_name)\n",
        "  header.append(\"FROST\")\n",
        "\n",
        "  # Prepare a list to hold the rows of the CSV\n",
        "  rows = []\n",
        "\n",
        "  # Find all the unique dates by iterating over the parameter data\n",
        "  dates = set()\n",
        "  for feature_name, date_val_dict in data['properties']['parameter'].items():\n",
        "    for date in date_val_dict.keys():\n",
        "      dates.add(date)\n",
        "\n",
        "  # Sort dates to maintain chronological order\n",
        "  dates = sorted(dates)\n",
        "\n",
        "  # For each date, create a row with values for each feature\n",
        "  for date in dates:\n",
        "    # Fill in date, neighbor number (this is for the middle of the grid, so 5),reference name, latitude, and longitude values\n",
        "    row = [date, 5, name, latitude, longitude]\n",
        "    # Add features to data\n",
        "    for feature_name in data['properties']['parameter'].keys():\n",
        "        # Get the value for the feature for the current date\n",
        "        value = data['properties']['parameter'][feature_name].get(date, None)\n",
        "        row.append(value)\n",
        "\n",
        "        # Now we want to add the frost prediction output\n",
        "        if feature_name == 'T2M_MIN':\n",
        "            t2m = value\n",
        "        elif feature_name == 'T2MDEW':\n",
        "            dew = value\n",
        "\n",
        "    # Check if dew < 0, and if dew < t2m\n",
        "    if t2m < 0 and dew > t2m and dew < 0:\n",
        "        frost = 1\n",
        "    else:\n",
        "        frost = 0\n",
        "\n",
        "    # Add frost prediction to instance\n",
        "    row.append(frost)\n",
        "    # Add instance to the dataset\n",
        "    rows.append(row)\n",
        "\n",
        "  # Now we do this for each neighbor on the tile. We'll use a 3x3 neighborhood\n",
        "  rows = extract_neighbor_data(latitude - 0.5, longitude + 0.625, 1, name, rows)\n",
        "  rows = extract_neighbor_data(latitude, longitude + 0.625, 2, name, rows)\n",
        "  rows = extract_neighbor_data(latitude + 0.5, longitude + 0.625, 3, name, rows)\n",
        "  rows = extract_neighbor_data(latitude - 0.5, longitude, 4, name, rows)\n",
        "  rows = extract_neighbor_data(latitude + 0.5, longitude, 6, name, rows)\n",
        "  rows = extract_neighbor_data(latitude - 0.5, longitude - 0.625, 7, name, rows)\n",
        "  rows = extract_neighbor_data(latitude, longitude - 0.625, 8, name, rows)\n",
        "  rows = extract_neighbor_data(latitude + 0.5, longitude - 0.625, 9, name, rows)\n",
        "\n",
        "  # File name - Each city has an output file. These will be combined in another notebook\n",
        "  filename = f\"{name}_output.csv\"\n",
        "\n",
        "  # Open a CSV file for writing\n",
        "  with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header (features as columns and date as the first column)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # Write the rows (values for each date)\n",
        "    writer.writerows(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e95cd32f-2891-4c55-a552-84f6d3d03809",
      "metadata": {
        "id": "e95cd32f-2891-4c55-a552-84f6d3d03809"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}